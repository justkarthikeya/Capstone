# -*- coding: utf-8 -*-
"""churn_prediction_ml_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O_Xe0qnMqQL8yCBdFO7JJRDv8LWc7nuX

Block 0 — Install Dependencies
"""

!pip install --quiet shap lightgbm xgboost imbalanced-learn category_encoders optuna

"""Block 1 — Imports & Constants

"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, roc_curve, precision_recall_curve, auc
from imblearn.over_sampling import SMOTE

import category_encoders as ce
import shap
import lightgbm as lgb
import xgboost as xgb

RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)
sns.set(style="whitegrid")

"""Block 2 — Load Dataset

"""

DATA_PATH = "/content/WA_Fn-UseC_-Telco-Customer-Churn.csv"
df = pd.read_csv(DATA_PATH)
print(f"Dataset shape: {df.shape}")
df.head()

"""Block 3 — Data Cleaning

"""

for c in df.select_dtypes(include=['object']).columns:
    df[c] = df[c].astype(str).str.strip()

df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')
df['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)

service_cols = ['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']
for c in service_cols:
    df[c] = df[c].replace({'No internet service': 'No'})
df['MultipleLines'] = df['MultipleLines'].replace({'No phone service': 'No'})
df['SeniorCitizen'] = df['SeniorCitizen'].map({1: 'Yes', 0: 'No'})
df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})

print("Cleaned data sample:")
df.head()

"""Block 4 — Exploratory Data Analysis (EDA)

"""

plt.figure(figsize=(8, 6))
ax = sns.countplot(x='Churn', data=df, palette='viridis')
plt.title('Distribution of Customer Churn', fontsize=16)
plt.xlabel('Churn (1 = Yes, 0 = No)', fontsize=12)
plt.ylabel('Number of Customers', fontsize=12)

total = len(df['Churn'])
for p in ax.patches:
    percentage = f'{100 * p.get_height() / total:.1f}%'
    x = p.get_x() + p.get_width() / 2
    y = p.get_height()
    ax.annotate(percentage, (x, y), ha='center', va='bottom', fontsize=12)

plt.show()

fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 12))
fig.suptitle('Churn Rate by Customer Demographics', fontsize=20)

sns.countplot(x='gender', hue='Churn', data=df, ax=axes[0, 0], palette='pastel')
axes[0, 0].set_title('Churn by Gender')

sns.countplot(x='SeniorCitizen', hue='Churn', data=df, ax=axes[0, 1], palette='pastel')
axes[0, 1].set_title('Churn by Senior Citizen Status')

sns.countplot(x='Partner', hue='Churn', data=df, ax=axes[1, 0], palette='pastel')
axes[1, 0].set_title('Churn by Partner Status')

sns.countplot(x='Dependents', hue='Churn', data=df, ax=axes[1, 1], palette='pastel')
axes[1, 1].set_title('Churn by Dependents Status')

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20, 6))
fig.suptitle('Distribution of Numerical Features by Churn', fontsize=20)

sns.histplot(data=df, x='tenure', hue='Churn', multiple='stack', kde=True, ax=axes[0], palette='magma')
axes[0].set_title('Tenure Distribution')

sns.histplot(data=df, x='MonthlyCharges', hue='Churn', multiple='stack', kde=True, ax=axes[1], palette='magma')
axes[1].set_title('Monthly Charges Distribution')

sns.histplot(data=df, x='TotalCharges', hue='Churn', multiple='stack', kde=True, ax=axes[2], palette='magma')
axes[2].set_title('Total Charges Distribution')

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 7))
fig.suptitle('Churn Rate by Contract and Payment Method', fontsize=20)

sns.countplot(x='Contract', hue='Churn', data=df, ax=axes[0], palette='coolwarm')
axes[0].set_title('Churn by Contract Type')

sns.countplot(y='PaymentMethod', hue='Churn', data=df, ax=axes[1], palette='coolwarm')
axes[1].set_title('Churn by Payment Method')
axes[1].set_ylabel('Payment Method')

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

"""Block 5 — Feature Engineering

"""

def add_features(df):
    X = df.copy()
    service_cols = ['PhoneService','MultipleLines','OnlineSecurity','OnlineBackup',
                    'DeviceProtection','TechSupport','StreamingTV','StreamingMovies']
    X['ServicesCount'] = (X[service_cols] == 'Yes').sum(axis=1)
    X['ContractOrd'] = X['Contract'].map({'Month-to-month':0,'One year':1,'Two year':2})
    X['IsAutoPay'] = X['PaymentMethod'].str.contains('automatic', case=False, na=False).astype(int)
    X['SimpleCLV'] = X['MonthlyCharges'] * X['tenure']
    X['HasInternet'] = (X['InternetService'] != 'No').astype(int)
    return X

Xfull = add_features(df)
Xfull.head()

"""Block 6 — Prepare Data

"""

y = Xfull['Churn']
X = Xfull.drop(columns=['Churn', 'customerID'])

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE)

"""Block 7 — Preprocessing Pipeline

"""

num_cols = X.select_dtypes(include=[np.number]).columns.tolist()
cat_cols = X.select_dtypes(include=['object','category']).columns.tolist()

num_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

cat_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('target_encoder', ce.TargetEncoder())
])

preprocessor = ColumnTransformer([
    ('num', num_pipeline, num_cols),
    ('cat', cat_pipeline, cat_cols)
])

"""Block 8 — Apply Preprocessing & SMOTE

"""

from category_encoders import TargetEncoder
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler

num_imputer = SimpleImputer(strategy='median')
num_scaler = StandardScaler()

X_train_num = num_imputer.fit_transform(X_train[num_cols])
X_test_num = num_imputer.transform(X_test[num_cols])
X_train_num = num_scaler.fit_transform(X_train_num)
X_test_num = num_scaler.transform(X_test_num)

target_encoder = TargetEncoder(cols=cat_cols)
X_train_cat = target_encoder.fit_transform(X_train[cat_cols], y_train)
X_test_cat = target_encoder.transform(X_test[cat_cols])

X_train_proc = np.hstack([X_train_num, X_train_cat])
X_test_proc = np.hstack([X_test_num, X_test_cat])

smote = SMOTE(random_state=RANDOM_STATE)
X_train_bal, y_train_bal = smote.fit_resample(X_train_proc, y_train)

print(f"Preprocessing completed. Balanced training shape: {X_train_bal.shape}")

"""Block 9 — Train Models & Select Best (Enhanced)

"""

models = {
    "LogisticRegression": LogisticRegression(max_iter=1000, class_weight='balanced'),
    "RandomForest": RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=RANDOM_STATE, n_jobs=-1),
    "XGBoost": xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=RANDOM_STATE, n_jobs=-1),
    "LightGBM": lgb.LGBMClassifier(random_state=RANDOM_STATE, n_jobs=-1)
}

fitted_models = {}
roc_aucs = {}
predictions = {}

for name, model in models.items():
    print(f"\n--- Training {name} ---")
    model.fit(X_train_bal, y_train_bal)
    y_pred = model.predict(X_test_proc)
    y_proba = model.predict_proba(X_test_proc)[:,1]

    roc_auc_value = roc_auc_score(y_test, y_proba)

    print(f"ROC-AUC: {roc_auc_value:.4f}")

    fitted_models[name] = model
    roc_aucs[name] = roc_auc_value
    predictions[name] = {'pred': y_pred, 'proba': y_proba}

best_model_name = max(roc_aucs, key=roc_aucs.get)
best_model = fitted_models[best_model_name]
print(f"\n Best Model: {best_model_name} with ROC-AUC: {roc_aucs[best_model_name]:.4f}\n")

fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 12))
axes = axes.flatten()

for i, (name, model) in enumerate(fitted_models.items()):
    ax = axes[i]
    y_pred = predictions[name]['pred']
    cm = confusion_matrix(y_test, y_pred)

    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, cbar=False)
    ax.set_title(f'Confusion Matrix: {name}', fontsize=14)
    ax.set_xlabel('Predicted Label')
    ax.set_ylabel('True Label')

fig.suptitle('Model Confusion Matrices', fontsize=20, y=1.02)
plt.tight_layout()
plt.show()

print("\n" + "="*50)
print("Classification Reports")
print("="*50)
for name in fitted_models:
    print(f"\n--- {name} ---")
    print(classification_report(y_test, predictions[name]['pred'], target_names=['Stay', 'Churn']))

fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 7))

ax1.plot([0, 1], [0, 1], 'k--', label='No Skill')
for name, results in predictions.items():
    fpr, tpr, _ = roc_curve(y_test, results['proba'])
    roc_auc = auc(fpr, tpr)
    ax1.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})')
ax1.set_title('Receiver Operating Characteristic (ROC) Curves', fontsize=16)
ax1.set_xlabel('False Positive Rate')
ax1.set_ylabel('True Positive Rate')
ax1.legend()

for name, results in predictions.items():
    precision, recall, _ = precision_recall_curve(y_test, results['proba'])
    pr_auc = auc(recall, precision)
    ax2.plot(recall, precision, label=f'{name} (AUC = {pr_auc:.3f})')
ax2.set_title('Precision-Recall Curves', fontsize=16)
ax2.set_xlabel('Recall')
ax2.set_ylabel('Precision')
ax2.legend()

plt.tight_layout()
plt.show()

"""Block 10 — SHAP Global Explanation

"""

print("\n SHAP Global Explanation")

explainer = shap.Explainer(best_model, X_test_proc)
shap_values = explainer(X_test_proc[:200])

all_cols = num_cols + cat_cols
feature_names_map = {f"Feature {i}": col for i, col in enumerate(all_cols)}

shap_plot_df = pd.DataFrame(shap_values.values, columns=all_cols)

shap.summary_plot(shap_values.values, X_test_proc[:200], feature_names=all_cols, max_display=10, show=True)
plt.savefig("shap_summary_top10.png", bbox_inches='tight')
plt.close()
print("SHAP summary saved as shap_summary_top10.png")

"""Block 11 — User Input + Prediction + Counterfactuals (Enhanced)

"""

sample_user = {
    "gender": "Male",
    "SeniorCitizen": "No",
    "Partner": "Yes",
    "Dependents": "Yes",
    "tenure": 72,
    "PhoneService": "Yes",
    "MultipleLines": "Yes",
    "InternetService": "DSL",
    "OnlineSecurity": "Yes",
    "OnlineBackup": "Yes",
    "DeviceProtection": "Yes",
    "TechSupport": "Yes",
    "StreamingTV": "Yes",
    "StreamingMovies": "Yes",
    "Contract": "Two year",
    "PaperlessBilling": "No",
    "PaymentMethod": "Bank transfer (automatic)",
    "MonthlyCharges": 55.0,
    "TotalCharges": 3960.0
}

user_df = pd.DataFrame([sample_user])
user_df = add_features(user_df)

for c in X.columns:
    if c not in user_df.columns:
        user_df[c] = np.nan
user_df = user_df[X.columns]

user_num = num_imputer.transform(user_df[num_cols])
user_num = num_scaler.transform(user_num)
user_cat = target_encoder.transform(user_df[cat_cols])
user_proc = np.hstack([user_num, user_cat])

# Predict churn
proba = best_model.predict_proba(user_proc)[0,1]
label = "Churn" if proba >= 0.5 else "Stay"
print(f"Predicted Churn Probability for Sample User: {proba:.3f} → {label}")

# Counterfactual Interventions
print("\n Counterfactual Interventions:")
interventions = [
    ("Switch to Month-to-Month", {"Contract": "Month-to-month"}),
    ("Disable Auto-Pay (Electronic Check)", {"PaymentMethod": "Electronic check"}),
    ("Increase Monthly Charges by $20", {"MonthlyCharges": sample_user['MonthlyCharges'] + 20}),
    ("Remove Online Security Service", {"OnlineSecurity": "No"}),
    ("Decrease Tenure by 60 months", {"tenure": sample_user['tenure'] - 60})
]
intervention_results = {"Original": proba}
for desc, changes in interventions:
    new_input = sample_user.copy()
    new_input.update(changes)
    new_df = pd.DataFrame([new_input])
    new_df = add_features(new_df)
    for c in X.columns:
        if c not in new_df.columns: new_df[c] = np.nan
    new_df = new_df[X.columns]
    new_num = num_imputer.transform(new_df[num_cols])
    new_num = num_scaler.transform(new_num)
    new_cat = target_encoder.transform(new_df[cat_cols])
    new_proc = np.hstack([new_num, new_cat])
    new_proba = best_model.predict_proba(new_proc)[0,1]
    delta = new_proba - proba
    intervention_results[desc] = new_proba
    print(f"If {desc}: churn prob {new_proba:.3f} (Δ {delta:+.3f})")

plt.figure(figsize=(12, 8))
intervention_df = pd.DataFrame(list(intervention_results.items()), columns=['Intervention', 'ChurnProbability'])
ax = sns.barplot(x='ChurnProbability', y='Intervention', data=intervention_df, palette='viridis')
plt.title('Impact of Interventions on Churn Probability', fontsize=16)
plt.xlabel('Predicted Churn Probability', fontsize=12)
plt.ylabel('')
plt.xlim(0, max(intervention_df['ChurnProbability']) * 1.1)
for p in ax.patches:
    ax.annotate(f'{p.get_width():.3f}', (p.get_width(), p.get_y() + p.get_height() / 2),
                ha = 'left', va = 'center', xytext = (5, 0), textcoords = 'offset points')
plt.show()

print("\n\n SHAP Local Explanation (Why did the model predict 'Stay'?)")
explainer = shap.Explainer(best_model.predict, X_test_proc)
shap_values = explainer(user_proc)

shap_explanation = shap.Explanation(
    values=shap_values.values[0],
    base_values=shap_values.base_values[0],
    data=user_proc[0],
    feature_names=all_cols
)

shap.plots.waterfall(shap_explanation, max_display=10)

"""Block 12 — Save Model Artifacts

"""

import joblib

joblib.dump(best_model, 'churn_model.pkl')
joblib.dump(num_imputer, 'num_imputer.pkl')
joblib.dump(num_scaler, 'num_scaler.pkl')
joblib.dump(target_encoder, 'target_encoder.pkl')

joblib.dump(num_cols, 'num_cols.pkl')
joblib.dump(cat_cols, 'cat_cols.pkl')